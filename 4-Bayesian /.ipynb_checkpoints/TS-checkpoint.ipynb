{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bandit:\n",
    "    \n",
    "    def __init__(self,prob):\n",
    "        self.prob = prob\n",
    "        self.qValue = 0\n",
    "        self.count = 0\n",
    "        \n",
    "    def pull(self):\n",
    "        return np.random.random()<self.prob\n",
    "    \n",
    "    \n",
    "    def update(self,reward):\n",
    "        self.count = self.count + 1\n",
    "        self.qValue = self.qValue + ( (reward - self.qValue)/self.count)\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## UCB ALGORITHM ################\n",
    "def UCB(qValues,counts,N):\n",
    "    arm = np.argmax(qValues+np.sqrt(2*np.log(N)/counts))    \n",
    "    return arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TEST CASE#################\n",
    "NUM_TRIALS = 10000\n",
    "banditsProbs = [0.2, 0.5, 0.75]\n",
    "rewards = np.zeros(NUM_TRIALS)\n",
    "regrets = np.zeros(NUM_TRIALS)\n",
    "\n",
    "\n",
    "bandits = [bandit(x) for x in banditsProbs]\n",
    "optimalArm = np.argmax([b.prob for b in bandits])\n",
    "\n",
    "totalPlays=0\n",
    "#init : for handling the log 0 and N=0 we pull each bandit once before running UCB1\n",
    "for i in range(np.size(banditsProbs)):\n",
    "    curReward = bandits[i].pull()\n",
    "    totalPlays+=1\n",
    "    bandits[i].update(curReward)\n",
    "i=1\n",
    "while i < NUM_TRIALS :\n",
    "    arm = UCB([b.qValue for b in bandits],[b.count for b in bandits],totalPlays)\n",
    "    curReward = bandits[arm].pull()\n",
    "    totalPlays+=1\n",
    "    bandits[arm].update(curReward)\n",
    "    rewards[i] = curReward\n",
    "    regrets[i] = banditsProbs[optimalArm] - banditsProbs[arm]\n",
    "    i=i+1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Q values: \" ,[b.qValue for b in bandits])\n",
    "print(\"Best arm out of\",len(bandits) ,\"arm is: \", np.argmax([b.qValue for b in bandits])+1 )\n",
    "print(\"num of times we've selected the bandits: \",[b.count for b in bandits])        \n",
    "print(\"total reward is: \",rewards.sum())\n",
    "print(\"Win rate: \",rewards.sum()/NUM_TRIALS)\n",
    "\n",
    "cumulativeRewards = np.cumsum(rewards)\n",
    "winRates = cumulativeRewards/(np.arange(NUM_TRIALS)+1)\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(winRates)\n",
    "plt.xlabel(\"TimeSteps\")\n",
    "plt.ylabel(\"Cumulative Win Rates\")\n",
    "plt.plot(np.ones(NUM_TRIALS)*np.max(banditsProbs))\n",
    "\n",
    "regrets = np.cumsum(regrets) #cumulative rewards loss due to the need of learning\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(regrets)\n",
    "plt.xlabel(\"TimeSteps\")\n",
    "plt.ylabel(\"Cumulative Regrets\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
